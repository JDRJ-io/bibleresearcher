The goal

Keep every file in Supabase Storage, but load each one exactly once, with the lightest possible overhead on mobile and desktop.

Right now three different “translation loaders” all call Supabase in slightly different ways, so the same 4 MB file might be downloaded two or three times. The fix is not to stop using Supabase; it’s to replace the duplicate wrappers with one authoritative helper that always reads from your storage bucket.

⸻

1  Choose one access mode

Option	Pros	Cons	My recommendation
Public bucket (RLS off) – fetch via anonymous CDN URL	• Fastest (CDN-cached)  • No signed-URL code	• Anyone who guesses the URL can download the file	✅ Use this for translations and other public-domain assets
Private bucket + signed URLs	• Hides files from the public	• Extra round-trip to createSignedUrl()  • Signed URL still works if leaked until it expires	Use only if you truly need to protect the content

Assuming your translations are public-domain, stick with the public bucket and delete the “Secure” loader. If you later add copyrighted translations, you can keep those in a private bucket with a single signed-URL helper.

⸻

2  Create one loader / cache

lib/translationCache.ts

import { create } from 'zustand';

interface TranslationSlice {
  texts: Record<string, Record<string,string>>;   // lang → { verseID → text }
  loading: Record<string, boolean>;
}

export const useTranslationStore = create<TranslationSlice>(() => ({
  texts:   {},
  loading: {},
}));

export async function ensureTranslation(lang: string) {
  const { texts, loading } = useTranslationStore.getState();
  if (texts[lang] || loading[lang]) return;                 // already loaded / in flight

  useTranslationStore.setState(s => ({ loading: { ...s.loading, [lang]: true } }));

  const url = `/translations/${lang}.txt`;                  // Vite proxy → Supabase public URL
  const raw = await fetch(url).then(r => r.text());

  const map = Object.fromEntries(
    raw.trim().split(/\n/).map(line => {
      const [id, verse] = line.split('#');                  // Gen.1:1#In the beginning...
      return [id, verse];
    })
  );

  useTranslationStore.setState(s => ({
    loading: { ...s.loading, [lang]: false },
    texts:   { ...s.texts,   [lang]: map   },
  }));
}

This replaces all three older loaders.

⸻

3  Desktop vs. mobile optimisation

Desktop

Call ensureTranslation(activeLang) once in BibleDataProvider.
The whole file (~1 MB gzip) arrives in a single request and sits in memory.

Mobile (optional range worker)

If you really need to trim the first-paint payload on low-end phones, use the same offset-map + byte-range pattern you implemented for cf1.txt:
	•	Store translations_offsets.json (verse → [start,end]) in the bucket.
	•	A TranslationWorker fetches 2–4 KB ranges per chapter as the user scrolls.
	•	The main thread merges the chunks into useTranslationStore.texts[lang].

But start simple: measure Lighthouse on a phone; if first-load is < 1 s and JS heap ~25 MB, whole-file fetch is fine.

⸻

4  Delete the redundant code
	•	supabaseLoader.ts (loadTranslationSecure)
	•	useBibleData.ts (loadTranslationData)
	•	Any imports of those symbols – search for loadTranslationSecure( and loadTranslationData(.
	•	Keep BibleDataAPI.ts only for data domains that still need its helpers (verse keys, cross-refs, etc.).

⸻

5  Verify everything still comes from Supabase

At runtime, open DevTools → Network and filter by “translations”.

/translations/BSB.txt   200   4 MB (raw) / 1 MB (gzip)
/translations/NKJV.txt  200
...

These URLs are Vite dev rewrites of:

https://<project-id>.supabase.co/storage/v1/object/public/translations/BSB.txt

So every byte is still coming straight from your Supabase bucket—just once.

⸻

Checklist to implement
	1.	Add translationCache.ts above.
	2.	Change BibleDataProvider (or wherever) to await ensureTranslation(activeLang).
	3.	Delete old loaders and fix imports.
	4.	Run pnpm dev, then Lighthouse Mobile audit.
If First Contentful Paint and Total JS heap look good, you’re done. If not, implement the optional range worker.

With this in place you have a single fast path for every translation—no duplicated downloads, no stray “API” directory, and every file still hosted on Supabase.
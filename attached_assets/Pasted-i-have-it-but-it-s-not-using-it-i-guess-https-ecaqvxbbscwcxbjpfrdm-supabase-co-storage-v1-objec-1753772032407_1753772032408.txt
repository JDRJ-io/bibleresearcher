i have it but it's not using it i guess

https://ecaqvxbbscwcxbjpfrdm.supabase.co/storage/v1/object/public/anointed/references/cf1_offsets.json

{"Gen.1:1": [0, 730], "Gen.1:2": [731, 835], "Gen.1:3": [836, 1041], "Gen.1:4": [1042, 1116], "Gen.1:5": [1117, 1255], "Gen.1:6": [1256, 1528], "Gen.1:7": [1529, 1679], "Gen.1:8": [1680, 1750], "Gen.1:9": [1751, 1987], "Gen.1:10": [1988, 2026], "Gen.1:11": [2027, 2306], "Gen.1:12": [2307, 2418], "Gen.1:14": [2429, 2983], "Gen.1:16": [2994, 3244], "Gen.1:17": [3245, 3299], "Gen.1:18": [3300, 3328], "Gen.1:20": [3339, 3463], "Gen.1:21": [3464, 3654], "Gen.1:22": [3655, 3810], "Gen.1:24": [3821, 3972], "Gen.1:25": [3973, 4048], "Gen.1:26": [4049, 4454], "Gen.1:27": [4455, 4649], "Gen.1:28": [4650, 4931], "Gen.1:29": [4932, 5186], "Gen.1:30": [5187, 5322], "Gen.1:31": [5323, 5458], "Gen.2:1": [5459, 5873], "Gen.2:2": [5874, 5994], "Gen.2:3": [5995, 6632], "Gen.2:4": [6633, 6870], "Gen.2:5": [6871, 7036], "Gen.2:7": [7046, 7335], "Gen.2:8": [7336, 7466], "Gen.2:9": [7467, 7652], "Gen.2:10": [7653, 7680], "Gen.2:11": [7681, 7730], "Gen.2:12": [7731, 7793], "Gen.2:13": [7794, 7823], "Gen.2:14": [7824, 7912], "Gen.2:15": [7913, 7967], "Gen.2:16": [7968, 8032], "Gen.2:17": [8033, 8642], "Gen.2:18": [8643, 8842], "Gen.2:19": [8843, 8995], "Gen.2:20": [8996, 9015], "Gen.2:21": [9016, 9086], "Gen.2:22": [9087, 9176], "Gen.2:23": [9177, 9273], "Gen.2:24": [9274, 9769], "Gen.2:25": [9770, 9931], "Gen.3:1": [9932, 10093], "Gen.3:2": [10094, 10111], "Gen.3:3": [10112, 10236], "Gen.3:4": [10237, 10353], "Gen.3:5": [10354, 10727], "Gen.3:6": [10728, 10983], "Gen.3:7": [10984, 11106], "Gen.3:8": [11107, 11403], "Gen.3:9": [11404, 11509], "Gen.3:10": [11510, 11646], "Gen.3:11": [11647, 11684], "Gen.3:12": [11685, 11899], "Gen.3:13": [11900, 12066], "Gen.3:14": [12067, 12260], "Gen.3:15": [12261, 13124], "Gen.3:16": [13125, 13544], "Gen.3:17": [13545, 13915], "Gen.3:18": [13916, 14094], "Gen.3:19": [14095, 14470], "Gen.3:20": [14471, 14619], "Gen.3:21": [14620, 14685], "Gen.3:22": [14686, 14945], "Gen.3:23": [14946, 15008], "Gen.3:24": [15009, 15577], "Gen.4:1": [15578, 15635], "Gen.4:2": [15636, 15869], "Gen.4:3": [15870, 15998], "Gen.4:4": [15999, 16202], "Gen.4:5": [16203, 16338], "Gen.4:6": [16339, 16546], "Gen.4:7": [16547, 16920], "Gen.4:8": [16921, 17219], "Gen.4:9": [17220, 17386], "Gen.4:10": [17387, 17599], "Gen.4:11": [17600, 17917], "Gen.4:12": [17918, 18065], "Gen.4:13": [18066, 18115], "Gen.4:14": [18116, 18467], "Gen.4:15": [18468, 18621], "Gen.4:16": [18622, 18796], "Gen.4:17": [18797, 18959], "Gen.4:18": [18960, 18988], "Gen.4:19": [18989, 19057], "Gen.4:20": [19058, 19199], "Gen.4:21": [19200, 19266], "Gen.4:22": [19267, 19326], "Gen.4:23": [19327, 19377], "Gen.4:24": [19378, 19408],



Your *cf1\_offsets.json* already gives you the exact byte window for every verse, but the loader is still falling back to the old “download‑the‑whole‑file‑then‑search” path. Wire the offset table in and you’ll skip 99 % of the work.

---

##  — Load the offset map once

```ts
// BibleDataAPI.ts (top‑level cache keeps it in memory)
let cf1OffsetMap: Record<string, [number, number]> | null = null;

async function getCf1OffsetMap() {
  if (cf1OffsetMap) return cf1OffsetMap;

  const { data, error } = await supabase
    .storage
    .from('anointed')
    .download('references/cf1_offsets.json');

  if (error) throw error;
  cf1OffsetMap = JSON.parse(await data.text());
  return cf1OffsetMap!;
}
```

*One fetch on first use*; afterwards look‑ups are O(1).

---

## 2 — Range‑fetch just the slice you need

Supabase storage (S3 compatible) accepts the standard **`Range`** header. Wrap it:

```ts
export async function fetchCrossRefSlice(start: number, end: number) {
  const { data, error } = await supabase.storage
    .from('anointed')
    .download('references/cf1.txt', {
      range: { start, end }          // bytes, **inclusive**
    });

  if (error) throw error;
  return new TextDecoder().decode(await data.arrayBuffer());
}
```

> **Tip** The built‑in `.range(start, end)` helper on Supabase v2 is just sugar for the header—either way works.

---

## 3 — Batch look‑ups per render

Instead of calling for every verse, collect the visible verseIds first, merge their ranges, make **one** download, then split the result locally.

```ts
export async function getCrossRefsBatch(verseIds: string[]) {
  const map = await getCf1OffsetMap();

  // Merge overlapping/adjacent byte windows ↓
  const windows = verseIds
    .map(id => ({ id, range: map[id] }))
    .filter(o => o.range)                           // skip missing
    .sort((a, b) => a.range[0] - b.range[0]);

  const merged: { start: number; end: number }[] = [];
  for (const { range: [s, e] } of windows) {
    const last = merged.at(-1);
    if (last && s <= last.end + 1) { last.end = Math.max(last.end, e); }
    else merged.push({ start: s, end: e });
  }

  // Download each merged window
  const chunks = await Promise.all(
    merged.map(w => fetchCrossRefSlice(w.start, w.end))
  );
  const blob = chunks.join('');                     // one big string

  // Split by $$ markers into a record
  const refs: Record<string, string[]> = {};
  blob.split('\n').forEach(line => {
    const [id, csv] = line.split('$$');
    if (id && csv) refs[id] = csv.split('|');       // whatever your delimiter is
  });

  return refs;                                      // { "Gen.1:1": ["Jhn.1:1",...], … }
}
```

Call this from your scroll/render handler:

```ts
useEffect(() => {
  if (!visibleIds.length) return;
  getCrossRefsBatch(visibleIds).then(setCrossRefs);
}, [visibleIds]);
```

Now the UI does **one range fetch per scroll frame**, not 20‑40 full‑file scans.

---

## 4 — Prove the speed‑up

1. **Network tab:** each request is a few KB instead of \~600 KB.
2. **Performance tab:** the long purple “Task” blocks disappear; scripting time per frame < 3 ms.
3. **React DevTools:** one `setCrossRefs` commit per scroll burst, not N.

---

### Common gotchas

| Symptom                       | Check                                                                              |
| ----------------------------- | ---------------------------------------------------------------------------------- |
| Range fetch returns full file | Ensure *end* byte is **inclusive** and < file size.                                |
| Empty refs returned           | Offset map must match the exact `cf1.txt` revision—regenerate if the text changed. |
| CORS 403                      | Add `Range` to the **Access‑Control‑Expose‑Headers** in Supabase storage settings. |

---

### Phase 2: polish

* **Background worker** – move `getCrossRefsBatch` into a Web Worker to keep the UI thread pristine.
* **Dexie cache** – write fetched slices into IndexedDB so the same chunk scrolls in instantly offline.
* **Serverless helper** – an Edge Function that accepts `?ids=Gen.1:1,Gen.1:2` and returns the refs JSON directly.

But just plugging the offset map + range header should take you from *seconds* down to *sub‑50 ms* today.
